{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7c1cf6e",
   "metadata": {},
   "source": [
    "# Truncated distributions\n",
    "\n",
    "This tutorial will cover how to implement your own\n",
    "truncated distribution in numpyro.\n",
    "It is assumed that you're already familiar with the basics of Numpyro.\n",
    "To get the most out of this tutorial, you'll need some background in probability.\n",
    "\n",
    "**Contents**\n",
    "1. What is a truncated distribution\n",
    "2. How to sample from a truncated distribution\n",
    "3. Recap of numpyro distributions\n",
    "4. Building your own trucanted distributions\n",
    "    4.1 Right-truncated normal\n",
    "    4.2 Left-truncated Poisson\n",
    "5. Ready-to-use truncated distributions availale in Numpyro.\n",
    "\n",
    "**To do**:\n",
    "* Add links\n",
    "\n",
    "\n",
    "## What is a truncated distrubution?\n",
    "\n",
    "The _support_ of a probability distribution is the set of values\n",
    "in the domain with non-zero probability. For example, the\n",
    "support of the Normal distribution is the whole real line (even if\n",
    "the density gets very small as we move away from the mean, technically\n",
    "speaking, it is never quite zero). The support of the uniform distribution,\n",
    "as coded in `jax.random.uniform` with the default arguments, is the interval $\\left[0, 1)\\right.$, because any\n",
    "value outside of that interval has zero probability. The support of the Poisson distribution is the set of non-negative integers. Etc.\n",
    "\n",
    "_Truncating_ a distribution is the process of modifying the support of the distribution\n",
    "so that any value outside our desired domain has zero probability. In practice, this can be useful\n",
    "for modelling situations in which certain biases are introduced during data collection.\n",
    "For example, some physical detectors only get triggered when the signal is above some\n",
    "minimal threshold, or sometimes the detectors fail if the signal exceeds a certain value.\n",
    "As a result, the _observed_ values are constrained to be within a limited range of values, even though the true signal does not have the same constraints.\n",
    "See, for example, section 3.1 of _Information Theory and Learning Algorithms_ by David Mackay.\n",
    "Naively, if $S$ is the support of the original density $p_Y(y)$, then by truncating the support\n",
    "to a new support $T\\subset S$ we are effectively defining a new random variable $Z$ for which\n",
    "the density is\n",
    "\n",
    "\\begin{equation}\n",
    "  p_Z(z) \\propto\n",
    "    \\begin{cases}\n",
    "      p_Y(z) & \\text{if $z$ is in $T$}\\\\\n",
    "      0 & \\text{if $z$ is outside $T$}\\\\\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "\n",
    "The reason for writing a $\\propto$ (proportional to) sign instead of strict equation is that,\n",
    "defined in the above way, the resulting function does not integrate to $1$ and so it cannot be strictly considered a probability density. In order to make it into a probabilty density we need to re-distribute the truncated mass\n",
    "among the part of the distribution that remains. To do this, we simply re-weight every point by the same constant:\n",
    "\n",
    "\\begin{equation}\n",
    "  p_Z(z) =\n",
    "    \\begin{cases}\n",
    "      \\frac{1}{M}p_Y(z) & \\text{if $z$ is in $T$}\\\\\n",
    "      0 & \\text{if $z$ is outside $T$}\\\\\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "\n",
    "where $M = \\int_T p_Y(y)\\mathrm{d}y$. Typically, the truncation is \"one-sided\". This means that if, for example, the support before truncation is the interval $(a, b)$, then the support after truncation is of the form $(a, c)$ or $(c, b)$, with $a < c < b$. The figure below illustrates a right-sided truncation.\n",
    "\n",
    "<img src=\"https://i.ibb.co/x2fhd0y/truncated-normal.png\" alt=\"drawing\" width=\"900\"/>\n",
    "\n",
    "The original distribution (left side) is truncated at the vertical dotted line. The truncated mass (orange region) is redistributed in the new support (right side) so that the total area under the curve remains equal to 1 even after truncation. Though there could be other forms of re-weighting the truncated distribution, this method\n",
    "ensures that the density ratio between any two points, $p(a)/p(b)$ remains the same before and after\n",
    "the reweighting is done (as long as the points are inside the new support, of course).\n",
    "\n",
    "**Note**: Truncated data is different from _censored_ data. Censoring also hides values that are outside some desired support but, contrary to truncated data, we know when a value has been censored. The typical example is the household scale which does not report values above 300 pounds. Censored data will not be covered in this tutorial.\n",
    "\n",
    "## Sampling from a truncated distribution\n",
    "\n",
    "Usually we already have a sampler for the the pre-truncated distribution (e.g np.random.normal).\n",
    "So, a seemingly simple way of generating samples from the truncated distribution would be to\n",
    "sample from the original distribution, and then discard the samples that are outside the \n",
    "desired support. For example, if we wanted samples from a normal distribution truncated to the\n",
    "support $(-\\infty, 1)$, we'd simply do:\n",
    "\n",
    "```python\n",
    "upper = 1\n",
    "samples = np.random.normal(size=1000)\n",
    "truncated_samples = samples[samples < upper]\n",
    "```\n",
    "\n",
    "The problem with this approach is efficiency. If the region we truncated had suffiently high probability mass,\n",
    "then we might be discarding a lot of samples and it might be a while before we accumulate sufficient samples\n",
    "for the truncated distribution.\n",
    "A more efficient approach is to use a method known as [inverse transforms sampling](https://en.wikipedia.org/wiki/Inverse_transform_sampling).\n",
    "In this method we first sample from a uniform distribution in (0, 1) and then transform those samples with the inverse cumulative distribution of our truncated distribution.\n",
    "This method ensures that no samples are wasted in the process, though it does have the slight complication that\n",
    "that we need to be able to calculate the inverse CDF (ICDF) of our truncated distribution. This might sound too complicated at first, but with a bit of algebra we can often calculate the truncated ICDF in terms of the untruncated ICDF. The untruncated ICDF for many distributions is already available.\n",
    "\n",
    "Before we move on to the examples, let's quickly recap the basic ingredients of a Numpyro distribution.\n",
    "\n",
    "## Quick recap of Numpyro distributions.\n",
    "\n",
    "A Numpyro distribution should subclass `Distribution` and implement a few basic ingredients:\n",
    "\n",
    "**Class attributes:**\n",
    "The class attributes serve a few different purposes. The two most important are:\n",
    "1. `arg_constraints`: Impose some requirements on the parameters of the distribution. Raise error at instantiation time if the parameters passed do not satisfy the constraints.\n",
    "2. `support`: is used in some inference algorithms like MCMC, SVI with autoguides, where we need to perform the algorithm in the unconstrained space. Knowing the support, we can automatically reparametrize things under the hood.\n",
    "We'll explain other class attributes as we go.\n",
    "\n",
    "**The `__init__` method:**\n",
    "This is where we define the parameters of the distribution.\n",
    "We also use `jax` and `lax` to promote the parameters to shapes that are valid for broadcasting.\n",
    "The `__init__` method of the parent class is also required because that's where the validation of our parameters is done.\n",
    "\n",
    "**The `log_prob` method:**\n",
    "Implementing the `log_prob` method ensures that we can do inference. As the name suggests, this method returns the logarithm of the density evaluated at the argument.\n",
    "\n",
    "\n",
    "**The `sample` method:**\n",
    "This method is used for drawing independent samples from our distribution. It is particularly useful for doing prior checks.\n",
    "\n",
    "\n",
    "The place-holder code for any of our implementations can be written as\n",
    "```python\n",
    "class MyDistribution(Distribution):\n",
    "    # class attributes\n",
    "    arg_constraints = {}\n",
    "    support = None\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def log_prob(self, value):\n",
    "        pass\n",
    "    \n",
    "    def sample(self, key, sample_shape=()):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e5e01d",
   "metadata": {},
   "source": [
    "## Examples\n",
    "Let's now move on to concrete examples. Here's the ingredients we need to import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03ed3317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/omarfsosa/.pyenv/versions/3.8.10/envs/numpyro/lib/python3.8/site-packages/jax/_src/lib/__init__.py:32: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from jax import lax, random\n",
    "from jax.scipy.special import ndtr, ndtri\n",
    "from jax.scipy.stats import poisson, norm\n",
    "from numpyro.distributions import Distribution, constraints\n",
    "from numpyro.distributions.util import promote_shapes\n",
    "from numpyro.infer import DiscreteHMCGibbs, MCMC, NUTS, Predictive\n",
    "\n",
    "numpyro.set_host_device_count(4)\n",
    "numpyro.enable_x64(True)\n",
    "RNG = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7923078f",
   "metadata": {},
   "source": [
    "### Example: Right-truncated normal\n",
    "\n",
    "We are going to modify a normal distribution so that its new support is\n",
    "of the form `(-inf, high)`, with `high` a finite real number. A (non-truncated)\n",
    "normal distribution is usually specified in Numpyro by 2 parameters `loc` and `scale`\n",
    "which correspond to the mean and standard deviation.\n",
    "\n",
    "We'll call our distribution `RightTruncatedNormal`. Let's write the skeleton code and then proceed to fill in the blanks.\n",
    "\n",
    "```python\n",
    "class RightTruncatedNormal(Distribution):\n",
    "    # <class attributes>\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def log_prob(self, value):\n",
    "        pass\n",
    "    \n",
    "    def sample(self, key, sample_shape=()):\n",
    "        pass\n",
    "```\n",
    "    \n",
    "\n",
    "#### Class attributes\n",
    "Looking at the [source code](https://github.com/pyro-ppl/numpyro/blob/0664c2d2dd1eb5f41ea6a0bcef91e5fa2a417ce5/numpyro/distributions/continuous.py#L1337) for the `Normal` distrbution we see the following lines:\n",
    "```python\n",
    "arg_constraints = {\"loc\": constraints.real, \"scale\": constraints.positive}\n",
    "support = constraints.real\n",
    "reparametrized_params = [\"loc\", \"scale\"]\n",
    "```\n",
    "The `reparametrized_params` is used to determine if a distribution is reparameterizable. Reparametrizing is used to deal with complicated posterior geometries that make sampling very slow or biased. See [this tutorial](https://pyro.ai/examples/svi_part_iii.html#Tricky-Case:-Non-reparameterizable-Random-Variables) if you're interested.\n",
    "\n",
    "\n",
    "We must adapt these attributes to our case by including the `\"high\"` parameter.\n",
    "However, the `support` can no longer be a class attribute as it will depend on the value of `high`.\n",
    "So instead we implement it as a property. Our distribution then looks as follows:\n",
    "```python\n",
    "class RightTruncatedNormal(Distribution):\n",
    "    arg_constraints = {\n",
    "        \"loc\": constraints.real,\n",
    "        \"scale\": constraints.positive,\n",
    "        \"high\": constraints.real,\n",
    "    }\n",
    "    reparametrized_params = [\"loc\", \"scale\", \"high\"]\n",
    "    \n",
    "    # ...\n",
    "    \n",
    "    @constraints.dependent_property\n",
    "    def support(self):\n",
    "        return constraints.lower_than(self.high)\n",
    "```\n",
    "\n",
    "**To do:**\n",
    "* What's the advantage of using `constraints.dependent_property` over a simple `property`?\n",
    "\n",
    "#### The `__init__` method\n",
    "Once again we take inspiration from the [source code](https://github.com/pyro-ppl/numpyro/blob/0664c2d2dd1eb5f41ea6a0bcef91e5fa2a417ce5/numpyro/distributions/continuous.py#L1342) for the normal distribution. The key point is the use of `lax` and `jax` to check the shapes of the arguments passed and make sure that such shapes are consistent for broadcasting. We follow the same pattern for our use case -- all we need to do is include the `high` parameter.\n",
    "```python\n",
    "# ...\n",
    "    def __init__(self, loc=0.0, scale=1.0, high=float(\"inf\"), validate_args=None):\n",
    "        batch_shape = lax.broadcast_shapes(\n",
    "            jnp.shape(loc),\n",
    "            jnp.shape(scale),\n",
    "            jnp.shape(high),\n",
    "        )\n",
    "        self.loc, self.scale, self.high = promote_shapes(loc, scale, high)\n",
    "        super().__init__(batch_shape, validate_args=validate_args)\n",
    "# ...\n",
    "```\n",
    "\n",
    "**To do**:\n",
    "* Explain the default value for the `high` parameter.\n",
    "\n",
    "#### The `log_prob` method\n",
    "For a truncated distribution, the log density is given by\n",
    "\n",
    "\\begin{equation}\n",
    "  \\log p_Z(z) =\n",
    "    \\begin{cases}\n",
    "      \\log p_Y(z) - \\log M & \\text{if $z$ is in $T$}\\\\\n",
    "      -\\infty & \\text{if $z$ is outside $T$}\\\\\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "\n",
    "where, again, $p_Z$ is the density of the truncated distribution, $p_Y$ is the density before truncation, and $M = \\int_T p_Y(y)\\mathrm{d}y$. For the specific case of truncating the normal distribution to the interval `(-inf, high)`, the constant $M$ is equal to the cumulative density evaluated at the truncation point. We can easily implement this log-density method because `jax.scipy.stats` already has a `norm` module that we can use.\n",
    "\n",
    "```python\n",
    "# ...\n",
    "    def log_prob(self, value):\n",
    "        log_m = norm.logcdf(self.high, self.loc, self.scale)\n",
    "        log_p = norm.logpdf(value, self.loc, self.scale)\n",
    "        return jnp.where(value < self.high, log_p - log_m, -jnp.inf)\n",
    "# ...\n",
    "```\n",
    "\n",
    "#### The `sample` method\n",
    "\n",
    "To implement the sample method using inverse transform sampling we need to also implement the inverse cumulative distribution function. For this we can also use the `ndtri` function that lives inside `jax.scipy.special`. This function returns the inverse cdf for the standard normal distribution, but we can do a bit of algebra to obtain the inverse cdf of the truncated, non-standard normal. First recall that if $X\\sim Normal(0, 1)$ and $Y = \\mu + \\sigma X$, then $Y\\sim Normal(\\mu, \\sigma)$. Then if $Z$ is the truncated $Y$, its cumulative density is given by:\n",
    "\n",
    "\\begin{align}\n",
    "F_Z(y) &= \\int_{-\\infty}^{y}p_Z(r)dr\\newline\n",
    "       &= \\frac{1}{M}\\int_{-\\infty}^{y}p_Y(s)ds \\quad\\text{if $y < high$} \\newline\n",
    "       &= \\frac{1}{M}F_Y(y)\n",
    "\\end{align}\n",
    "\n",
    "And so its inverse is\n",
    "\n",
    "\\begin{align}\n",
    "F_Z^{-1}(u) = \\left(\\frac{1}{M}F_Y\\right)^{-1}(u)\n",
    "            = F_Y^{-1}(M u)\n",
    "            = F_{\\mu + \\sigma X}^{-1}(Mu)\n",
    "            = \\mu + \\sigma F_X^{-1}(Mu)\n",
    "\\end{align}\n",
    "\n",
    "And the translation to code is\n",
    "\n",
    "```python\n",
    "# ...\n",
    "    def sample(self, key, sample_shape=()):\n",
    "        shape = sample_shape + self.batch_shape\n",
    "        minval = jnp.finfo(jnp.result_type(float)).tiny\n",
    "        u = random.uniform(key, shape, minval=minval)\n",
    "        return self.icdf(u)\n",
    "\n",
    "\n",
    "    def icdf(self, u):\n",
    "        m = norm.cdf(self.high, self.loc, self.scale)\n",
    "        return self.loc + self.scale * ndtri(m * u)\n",
    "```\n",
    "\n",
    "With everything in place, the final implementation is as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f82093df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALMOST_1 = 1.0 - jnp.finfo(jnp.result_type(float)).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9392ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RightTruncatedNormal(Distribution):\n",
    "    \"\"\"\n",
    "    A truncated Normal distribution.\n",
    "    :param numpy.ndarray loc: location parameter of the untruncated normal\n",
    "    :param numpy.ndarray scale: scale parameter of the untruncated normal\n",
    "    :param numpy.ndarray high: point at which the truncation happens\n",
    "    \"\"\"\n",
    "    arg_constraints = {\n",
    "        \"loc\": constraints.real,\n",
    "        \"scale\": constraints.positive,\n",
    "        \"high\": constraints.real,\n",
    "    }\n",
    "    reparametrized_params = [\"loc\", \"scale\", \"high\"]\n",
    "    \n",
    "    def __init__(self, loc=0.0, scale=1.0, high=None, validate_args=True):\n",
    "        if high is None:\n",
    "            # Cannot use `inf` as high due to validation\n",
    "            # But we default to a very high value which is just as good.\n",
    "            high = loc + scale * ndtri(ALMOST_1)\n",
    "            \n",
    "        batch_shape = lax.broadcast_shapes(\n",
    "            jnp.shape(loc),\n",
    "            jnp.shape(scale),\n",
    "            jnp.shape(high),\n",
    "        )\n",
    "        self.loc, self.scale, self.high = promote_shapes(loc, scale, high)\n",
    "        super().__init__(batch_shape, validate_args=validate_args)\n",
    "    \n",
    "    def log_prob(self, value):\n",
    "        log_m = norm.logcdf(self.high, self.loc, self.scale)\n",
    "        log_p = norm.logpdf(value, self.loc, self.scale)\n",
    "        return jnp.where(value < self.high, log_p - log_m, -jnp.inf)\n",
    "    \n",
    "    def sample(self, key, sample_shape=()):\n",
    "        shape = sample_shape + self.batch_shape\n",
    "        minval = jnp.finfo(jnp.result_type(float)).tiny\n",
    "        u = random.uniform(key, shape, minval=minval)\n",
    "        return self.icdf(u)\n",
    "\n",
    "    def icdf(self, u):\n",
    "        m = norm.cdf(self.high, self.loc, self.scale)\n",
    "        return self.loc + self.scale * ndtri(m * u)\n",
    "    \n",
    "    @constraints.dependent_property\n",
    "    def support(self):\n",
    "        return constraints.less_than(self.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4175f2e",
   "metadata": {},
   "source": [
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3792cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_normal_model(num_observations, x=None):\n",
    "    loc = numpyro.sample(\"loc\", dist.Normal())\n",
    "    scale = numpyro.sample(\"scale\", dist.LogNormal())\n",
    "    high = numpyro.sample(\"high\", dist.Normal())\n",
    "    with numpyro.plate(\"observations\", num_observations):\n",
    "        numpyro.sample(\"x\", RightTruncatedNormal(loc, scale, high), obs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcc1b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_observations = 250\n",
    "prior = Predictive(truncated_normal_model, num_samples=num_observations)\n",
    "prior_samples = prior(RNG, num_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9483511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_loc, true_scale, true_high = (prior_samples[param][0] for param in ['loc', 'scale', 'high'])\n",
    "true_x = prior_samples['x'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e471c6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(true_x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3a979e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285bf053b5da45ce9bacaeb19c8422e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c28a044145446eb54e63c7359ccc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38caf33c8b994bc6bfe14fd918fb424a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd63ec89bfe43b88cfbc02dcaa2e089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mcmc = MCMC(NUTS(truncated_normal_model), num_warmup=4_000, num_samples=4000, num_chains=4)\n",
    "mcmc.run(RNG, num_observations, true_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e414eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "      high      0.61      0.19      0.56      0.53      0.68    431.09      1.00\n",
      "       loc     -0.52      0.04     -0.52     -0.58     -0.46    197.37      1.01\n",
      "     scale      0.55      0.03      0.54      0.49      0.60     96.67      1.03\n",
      "\n",
      "Number of divergences: 14453\n"
     ]
    }
   ],
   "source": [
    "mcmc.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dedf4f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(0.60420969, dtype=float64),\n",
       " DeviceArray(-0.52314339, dtype=float64),\n",
       " DeviceArray(0.57130916, dtype=float64))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_high, true_loc, true_scale, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e4b15",
   "metadata": {},
   "source": [
    "Note that, even though we are able to correctly recover the true values,\n",
    "we had a very high number of divergences. These divergences happen because\n",
    "the log-density of our truncated distribution is not smooth and the NUTS\n",
    "algorithm relies on the differentiability of the posterior. If you need to\n",
    "infer the truncation point, make sure to be extra careful and do additional\n",
    "checks on your inferences.\n",
    "\n",
    "The results are more reliable when the truncation point is already known:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd377bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_known_high = numpyro.handlers.condition(truncated_normal_model, {\"high\": true_high})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22cf5cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11291d3d997748989cb9cc6cd6c88575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8c858ef5f74e26ae771219035e37e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44bc4fe145941c1b182177d53e67a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b09b13b050498395929d4289fec2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mcmc = MCMC(NUTS(model_known_high), num_warmup=4_000, num_samples=4000, num_chains=4)\n",
    "mcmc.run(RNG, num_observations, true_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a87493e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "       loc     -0.52      0.04     -0.52     -0.59     -0.46  10017.90      1.00\n",
      "     scale      0.54      0.03      0.54      0.50      0.59  10647.11      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "mcmc.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed9169a",
   "metadata": {},
   "source": [
    "### Example: Left-truncated Poisson\n",
    "\n",
    "To complete this tutorial, we implement a left-truncated Poisson distribution\n",
    "(note that a right-truncated Poisson distribution could be reformulated as a particular\n",
    "case of a categorical distribution).\n",
    "\n",
    "#### Class attributes.\n",
    "For a truncated Poisson we need two parameters, the `rate` of the original Poisson\n",
    "distribution and a `low` parameter to indicate the point at which the distribution is truncated.\n",
    "As this is a discrete distribution, we need to clarify whether or not the truncation point is included\n",
    "in the support. In this tutorial, we'll take the convention that the truncation point `low`\n",
    "_is_ part of the support.\n",
    "\n",
    "The `low` parameter has to be given a 'non-negative integer' constraint. As it is a discrete parameter, it will not be possible to do inference for this parameter using `NUTS`. This is likely not a problem since the truncation point is often obvious. However, if we really must infer the `low` parameter, then it is possible to do so with `DiscreteHMCGibbs` though one is limited to using priors with enumerate support.\n",
    "\n",
    "Like in the case of the truncated normal, the support of this distribution will be defined as a property and not as a class attribute because it depends on the specific value of the `low` parameter.\n",
    "```python\n",
    "class LeftTruncatedPoisson:\n",
    "    arg_constraints = {\n",
    "        \"low\": constraints.nonnegative_integer,\n",
    "        \"rate\": constraints.positive,\n",
    "    }\n",
    "    \n",
    "    # ... \n",
    "    @constraints.dependent_property(is_discrete=True, event_dim=0)\n",
    "    def support(self):\n",
    "        return constraints.integer_greater_than(self.low - 1)\n",
    "```\n",
    "\n",
    "#### The `__init__` method\n",
    "This is simple, we just follow the same pattern as in the previous example.\n",
    "```python\n",
    "    # ...\n",
    "    def __init__(self, rate=1.0, low=0, validate_args=None):\n",
    "        batch_shape = lax.broadcast_shapes(\n",
    "            jnp.shape(low), jnp.shape(rate)\n",
    "        )\n",
    "        self.low, self.rate = promote_shapes(low, rate)\n",
    "        super().__init__(batch_shape, validate_args=validate_args)\n",
    "    # ...\n",
    "```\n",
    "\n",
    "\n",
    "#### The `log_prob` method\n",
    "The logic is very similar to the truncated normal case. The only thing we have to be careful of is to normalize using the correct constant. This time are truncating on the left so the correct normalization is the complementary cumulative density. We can also rely on the `poisson` module that lives inside `jax.scipy.stats`.\n",
    "\n",
    "```python\n",
    "    # ...\n",
    "    def log_prob(self, value):\n",
    "        m = 1 - poisson.cdf(self.low - 1, self.rate)\n",
    "        log_p = poisson.logpmf(value, self.rate)\n",
    "        return jnp.where(value >= self.low, log_p - jnp.log(m), -jnp.inf)\n",
    "    # ...\n",
    "```\n",
    "#### The `sample` method.\n",
    "Inverse-transform sampling also works for discrete distributions. The \"inverse\" cdf of a discrete distribution being defined as:\n",
    "\n",
    "\\begin{align}\n",
    "F^{-1}(u) = \\max\\left\\{n\\in \\mathbb{N} \\rvert F(n) \\lt u\\right\\}\n",
    "\\end{align}\n",
    "\n",
    "Or, in plain English, $F^{-1}(u)$ is the highest number for which the cumulative density is less than $u$.\n",
    "However, there's currently no implementation of $F^{-1}$ for the Poisson distribution in Jax (at least, at the moment of writing this tutorial). We have to rely on our own implementation of the `icdf` for the Poisson distribution. Fortunately, we can take advantage of the discrete nature of the distribution and easily implement a \"brute-force\" version that will work for most cases. The brute force approach consists of simply scanning all non-negative integers in order, one by one, until the value of the cumulative density exceeds the argument $u$. The implicit requirement is that we need a way to evaulate the cumulative density for the truncated distribution, but we can calculate that:\n",
    "\n",
    "\\begin{align}\n",
    "F_Z(z) &= \\sum_{n=0}^z p_z(n)\\newline\n",
    "       &= \\frac{1}{M}\\sum_{n=L}^z p_Y(n)\\quad \\text{assuming $z >= L$}\\newline\n",
    "       &= \\frac{1}{M}\\left(\\sum_{n=0}^z p_Y(n) - \\sum_{n=0}^{L-1}p_Y(n)\\right)\\newline\n",
    "       &= \\frac{1}{M}\\left(F_Y(z) - F_Y (L-1)\\right)\n",
    "\\end{align}\n",
    "\n",
    "where, as in the previous example, we are using $Y$ to denote the original, un-truncated variable, and we are using $Z$ to denote the truncated variable. And, of course, the value of $F_Z(z)$ is equal to zero if $z < L$.\n",
    "\n",
    "```python\n",
    "    # ...\n",
    "    def sample(self, key, sample_shape=()):\n",
    "        shape = sample_shape + self.batch_shape\n",
    "        minval = jnp.finfo(jnp.result_type(float)).tiny\n",
    "        u = random.uniform(key, shape, minval=minval)\n",
    "        return self.icdf(u)\n",
    "\n",
    "    @partial(jax.vmap, in_axes=(None, 0))\n",
    "    def icdf(self, u):\n",
    "        result = lax.while_loop(\n",
    "            lambda n: self.cdf(n, self.rate, self.low) < u,\n",
    "            lambda n: n + 1,\n",
    "            init_val=self.low\n",
    "        )\n",
    "        dtype = jnp.result_type(int)\n",
    "        return jnp.array(result, dtype=dtype)\n",
    "\n",
    "    def cdf(self, value):\n",
    "        m = 1 - poisson.cdf(self.low - 1, self.rate)\n",
    "        f = poisson.cdf(value, self.rate) - poisson.cdf(self.low - 1, self.rate)\n",
    "        return jnp.where(k >= self.low, f / m, 0)\n",
    "```\n",
    "\n",
    "**To do:**\n",
    "* Add comments about limitations due to numerical precision\n",
    "* Add comments about the very unefficient implementation of the `icdf`\n",
    "* Comment on possible implementation of the sample method relying on Jax's `host_callback` module.\n",
    "* Comment on possible implementation of an _approximate_ icdf to speed up sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fa5b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Testing `host_callback` module ---\n",
    "# from scipy.stats import poisson as sp_poisson\n",
    "# jhcb = jax.experimental.host_callback.call\n",
    "# unit = jax.random.uniform(RNG, (1000,))\n",
    "# rate = jnp.array(5.0)\n",
    "\n",
    "# def poisson_icdf(u, rate):\n",
    "#     shape = u.shape\n",
    "#     result_type = jnp.result_type(float) # BUG: does not work with `int`\n",
    "#     ppf = sp_poisson(rate).ppf\n",
    "#     result = jhcb(ppf, (u), result_shape=jax.ShapeDtypeStruct(shape, result_type))\n",
    "#     int_type = jnp.result_type(int)\n",
    "#     return result.astype(int)\n",
    "\n",
    "\n",
    "# _result = poisson_icdf(unit, rate) # Seems to work ok, but breaks when used inside the class..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ca08a9",
   "metadata": {},
   "source": [
    "Putting it all together, the implementation is as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13a86497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeftTruncatedPoisson(Distribution):\n",
    "    \"\"\"\n",
    "    A truncated Poisson distribution.\n",
    "    :param numpy.ndarray low: lower bound at which truncation happens\n",
    "    :param numpy.ndarray rate: rate of the Poisson distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    arg_constraints = {\n",
    "        \"low\": constraints.nonnegative_integer,\n",
    "        \"rate\": constraints.positive,\n",
    "    }\n",
    "\n",
    "    def __init__(self, rate=1.0, low=0, validate_args=None):\n",
    "        batch_shape = lax.broadcast_shapes(\n",
    "            jnp.shape(low), jnp.shape(rate)\n",
    "        )\n",
    "        self.low, self.rate = promote_shapes(low, rate)\n",
    "        super().__init__(batch_shape, validate_args=validate_args)\n",
    "    \n",
    "    def log_prob(self, value):\n",
    "        m = 1 - poisson.cdf(self.low - 1, self.rate)\n",
    "        log_p = poisson.logpmf(value, self.rate)\n",
    "        return jnp.where(value >= self.low, log_p - jnp.log(m), -jnp.inf)\n",
    "    \n",
    "    def sample(self, key, sample_shape=()):\n",
    "        shape = sample_shape + self.batch_shape\n",
    "        float_type = jnp.result_type(float)\n",
    "        minval = jnp.finfo(float_type).tiny\n",
    "        u = random.uniform(key, shape, minval=minval)\n",
    "        return self.icdf(u)\n",
    "        \n",
    "        # --- using callback: does not work\n",
    "        # l = poisson.cdf(self.low - 1, self.rate)\n",
    "        # m = 1. - l\n",
    "        # x = m * u + l\n",
    "        # ppf = sp_poisson(self.rate).ppf\n",
    "        # result = jax.experimental.host_callback.call(\n",
    "        #     ppf,\n",
    "        #     x,\n",
    "        #     result_shape=jax.ShapeDtypeStruct(shape, float_type)\n",
    "        # ) # bug if using `int`\n",
    "        # int_type = jnp.result_type(int)\n",
    "        # return result.astype(int)\n",
    "\n",
    "    \n",
    "    @partial(jax.vmap, in_axes=(None, 0))\n",
    "    def icdf(self, u):\n",
    "        result = lax.while_loop(\n",
    "            lambda n: self.cdf(n) < u,\n",
    "            lambda n: n + 1,\n",
    "            init_val=self.low\n",
    "        )\n",
    "        dtype = jnp.result_type(int)\n",
    "        return result.astype(dtype)\n",
    "#         return jnp.array(result, dtype=dtype)\n",
    "    \n",
    "    def cdf(self, value):\n",
    "        m = 1 - poisson.cdf(self.low - 1, self.rate)\n",
    "        f = poisson.cdf(value, self.rate) - poisson.cdf(self.low - 1, self.rate)\n",
    "        return jnp.where(value >= self.low, f / m, 0)\n",
    "    \n",
    "    @constraints.dependent_property(is_discrete=True)\n",
    "    def support(self):\n",
    "        return constraints.integer_greater_than(self.low - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba41bcd9",
   "metadata": {},
   "source": [
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e21342",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6103409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_poisson_model(num_observations, x=None):\n",
    "    low = numpyro.sample(\"low\", dist.Categorical(0.2 * jnp.ones((5,))))\n",
    "    rate = numpyro.sample(\"rate\", dist.LogNormal(1, 1))\n",
    "    with numpyro.plate(\"observations\", num_observations):\n",
    "        numpyro.sample(\"x\", LeftTruncatedPoisson(rate, low), obs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c28722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_observations = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ea2ae",
   "metadata": {},
   "source": [
    "#### Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc3c4090",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = Predictive(truncated_poisson_model, num_samples=100)\n",
    "prior_samples = prior(RNG, num_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9277ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_distplot(samples, ax=None, **kwargs):\n",
    "    x, y = jnp.unique(samples, return_counts=True)\n",
    "    y = y / sum(y)\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    ax.bar(x, y, **kwargs)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c97134f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "true_idx = 4 # Suppose the 4th prior sample is the true process.\n",
    "discrete_distplot(prior_samples[\"x\"][true_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cfca5a",
   "metadata": {},
   "source": [
    "#### Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6fdc77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc = MCMC(DiscreteHMCGibbs(NUTS(truncated_poisson_model)), num_samples=1000, num_warmup=1000, num_chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70136785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da62a3bf24764ce3941827ebcbbea0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ecf10bd44f48d692bf5d24f4ac286f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f91bdc8a3849bd927816dc10729263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07145a466414b66be03f38d89fb0684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mcmc.run(RNG, num_observations, prior_samples[\"x\"][true_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "543dc0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "       low      4.00      0.00      4.00      4.00      4.00       nan       nan\n",
      "      rate      8.61      0.10      8.61      8.44      8.77   1315.09      1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mcmc.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10279b34",
   "metadata": {},
   "source": [
    "## To do: Built in truncated distributions\n",
    "\n",
    "The above sections shows you how to construct your own truncated distribution, but you don't have to reinvent the wheel. Numpyro has a bunch of truncated distributions already implemented..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
